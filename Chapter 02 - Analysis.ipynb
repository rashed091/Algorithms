{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm is little more than a series of steps required to perform some task. If we treat each step as a basic unit of computation, then an algorithm’s execution time can be expressed as the *number of steps required to solve the problem*.\n",
    "\n",
    "This abstraction is exactly what we need: it characterizes an algorithm’s efficiency in terms of execution time while remaining independent of any particular program or computer. Now we can take a closer look at those two summation algorithms we introduced last chapter.\n",
    "\n",
    "Intuitively, we can see that the first algorithm (`sum_of_n`) is doing more work than the second (`arithmetic_sum`): some program steps are being repeated, and the program takes even longer if we increase the value of `n`. But we need to be more precise.\n",
    "\n",
    "The most expensive unit of computation in `sum_of_n` is variable assignment. If we counted the number of assignment statements, we would have a worthy approximation of the algorithm's execution time: there's an initial assignment statement (`total = 0`) that is performed only once, followed by a loop that executes the loop body (`total += i`) `n` times.\n",
    "\n",
    "We can denote this more succinctly with function $T$, where $T(n)=1+n$.\n",
    "\n",
    "The parameter $n$ is often referred to as the “size of the problem”, so we can read this as “$T(n)$ is the time it takes to solve a problem of size $n$, namely 1 + $n$ steps.”\n",
    "\n",
    "For our summation functions, it makes sense to use the number of terms being summed to denote the size of the problem. Then, we can say that “The sum of the first 100,000 integers is a *bigger instance* of the summation problem than the sum of the first 1,000 integers.”\n",
    "\n",
    "Based on that claim, it seems reasonable that the time required to solve the larger case would be greater than for the smaller case. “Seems reasonable” isn’t quite good enough, though; we need to _prove_ that the algorithm’s execution time depends on the size of the problem.\n",
    "\n",
    "To do this, we’re going to stop worrying about the *exact* number of operations an algorithm performs and determine the dominant part of the $T(n)$ function. We can do this because, as the problem gets larger, some portion of the $T(n)$ function tends to overpower the rest; it’s this dominant portion that is ultimately most helpful for algorithm comparisons.\n",
    "\n",
    "The *order of magnitude* function describes the part of $T(n)$ that increases fastest as the value of $n$ increases. “Order of magnitude function” is a bit of a mouthful, though, so we call it *big O*. We\n",
    "write it as $O(f(n))$, where $f(n)$ is the dominant part of the original $T(n)$. This is called “Big O notation” and provides a useful approximation for the *actual* number of steps in a computation.\n",
    "\n",
    "In the above example, we saw that $T(n)=1+n$. As $n$ gets larger, the constant 1 will become less significant to the final result. If we are simply looking for an approximation of $T(n)$, then we can drop the 1 and say that the running time is $O(n)$.\n",
    "\n",
    "Let’s be clear, though: the 1 *is* important to $T(n)$ and can only be safely ignored when we are looking for an *approximation* of $T(n)$.\n",
    "\n",
    "As another example, suppose that the exact number of steps in some algorithm is $T(n)=5n^{2}+27n+1005$. When $n$ is small (1 or 2), the constant 1005 seems to be the dominant part of the function. However, as $n$ gets larger, the term $n^{2}$ becomes enormous, dwarfing the other two terms in its contribution to the final result.\n",
    "\n",
    "Again, for an *approximation* of $T(n)$ at large values of $n$, we can focus on $5n^{2}$ and ignore the other terms. Similarly, the coefficient $5$ becomes insignificant as $n$ gets larger. We would then say that the function $T(n)$ has an order of magnitude $f(n)=n^{2}$; more simply, the function $T(n)$ is $O(n^{2})$.\n",
    "\n",
    "Although we don’t see this in the summation example, sometimes the performance of an algorithm depends on the problem’s **exact** data values rather than its size. For these kinds of algorithms, we need to characterize their performances as *worst case*, *best case*, or *average case*.\n",
    "\n",
    "The worst case performance refers to a particular data set where the algorithm performs especially poorly, while the best case performance refers to a data set that the algorithm can process extremely fast. The average case, as you can probably infer, performs somewhere in between these two extremes. Understanding these distinctions can help prevent any one particular case from misleading us.\n",
    "\n",
    "There are several common order of magnitude functions that will frequently appear as you study algorithms. These functions are listed below from lowest order of magnitude to highest. Knowing these goes a long way toward recognizing them in your own code.\n",
    "\n",
    "f(n) | Name\n",
    "--- | ---\n",
    "$1$ | Constant\n",
    "$\\log{n}$ | Logarithmic\n",
    "$n$ | Linear\n",
    "$n\\log{n}$ | Log Linear\n",
    "$n^{2}$ | Quadratic\n",
    "$n^{3}$ | Cubic\n",
    "$2^{n}$ | Exponential\n",
    "\n",
    "In order to decide which of these functions dominates a $T(n)$ function, we must see how it compares with the others as $n$ becomes larger. We’ve taken the liberty of graphing these functions together below.\n",
    "\n",
    "![](figures/big-o-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice that, when $n$ is small, the functions inhabit a similar area; it’s hard to tell which is dominant. However, as $n$ grows, they branch, making it easy for us to distinguish them.\n",
    "\n",
    "As a final example, suppose that we have the fragment of Python code shown below. Although this program does nothing useful, it’s instructive to see how we can take actual code and analyze its performance.\n",
    "\n",
    "```python\n",
    "a = 5\n",
    "b = 6\n",
    "c = 10\n",
    "for i in range(n):\n",
    "   for j in range(n):\n",
    "      x = i * i\n",
    "      y = j * j\n",
    "      z = i * j\n",
    "for k in range(n):\n",
    "   w = a * k + 45\n",
    "   v = b * b\n",
    "d = 33\n",
    "```\n",
    "\n",
    "To calculate $T(n)$ for this fragment, we need to count the assignment operations, which is easier if we group them logically.\n",
    "\n",
    "The first group consists of three assignment statements, giving us the term **3**. The second group consists of three assignments in the nested iteration: **$3n^{2}$**. The third group has two assignments iterated $n$ times: **$2n$**. The fourth “group” is the last assignment statement, which is just the constant **1**.\n",
    "\n",
    "Putting those all together: $T(n)=3+3n^{2}+2n+1=3n^{2}+2n+4$. By looking at the exponents, we can see that the $n^{2}$ term will be dominant, so this fragment of code is $O(n^{2})$. Remember that we can safely ignore all the terms and coefficients as $n$ grows larger.\n",
    "\n",
    "The diagram below shows a few of the common big O functions as they compare with the $T(n)$ function discussed above.\n",
    "\n",
    "![](figures/big-o-plot-2.png)\n",
    "\n",
    "Note that $T(n)$ is initially larger than the cubic function but, as $n$ grows, $T(n)$ cannot compete with the rapid growth of the cubic function. Instead, it heads in the same direction as the quadratic function as $n$ continues to grow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Anagram Detection Example\n",
    "To explore different orders of magnitude, let’s consider four different solutions to the problem of detecting if a string is an anagram. One string is an anagram of another if the second is a rearrangement of the letters in the first. For example, `'heart'` and `'earth'` are anagrams.\n",
    "\n",
    "For the sake of simplicity, let’s assume that the two strings in question use symbols from the set of 26 lowercase alphabetic characters. Our goal is to write a boolean function that will take two strings and return whether they are anagrams.\n",
    "\n",
    "#### Solution 1: Checking Off\n",
    "\n",
    "Our first solution to the anagram problem will check whether each character in the first string occurs in the second. As we perform these checks, we’ll “check off” characters. If we can check off each character, then the two strings must be anagrams.\n",
    "\n",
    "We can check off a character by replacing it with the special Python value None. Since strings in Python are immutable, we need to convert the second string to a list. Each character from the first string will be checked against the characters in this list and, if found, checked off by replacement.\n",
    "\n",
    "An implementation of this strategy might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def anagram_checking_off(s1, s2):\n",
    "    if len(s1) != len(s2):\n",
    "        return False\n",
    "\n",
    "    to_check_off = list(s2)\n",
    "\n",
    "    for char in s1:\n",
    "        for i, other_char in enumerate(to_check_off):\n",
    "            if char == other_char:\n",
    "                to_check_off[i] = None\n",
    "                break\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "anagram_checking_off('abcd', 'dcba')  # => True\n",
    "anagram_checking_off('abcd', 'abcc')  # => False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze this algorithm, note that each of the $n$ characters in `s1` causes an iteration of up to $n$ characters in the list from `s2`. Each of the $n$ positions in the list will be visited once to match a character from `s1`.\n",
    "\n",
    "So the number of visits becomes the sum of the integers from 1 to $n$. We recognized earlier that this can be written as\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} i = \\frac {n(n+1)}{2} = \\frac {1}{2}n^{2} + \\frac {1}{2}n\n",
    "$$\n",
    "\n",
    "As $n$ gets larger, the $n^{2}$ term will dominate the $n$ term and the $\\frac {1}{2}$ constant can be ignored. Therefore, this solution is $O(n^{2})$.\n",
    "\n",
    "#### Solution 2: Sort and Compare\n",
    "\n",
    "A second solution uses the fact that, even though `s1` and `s2` are different, they are only anagrams if they consist of the same characters. If the strings are anagrams, sorting them both alphabetically should produce the same string.\n",
    "\n",
    "First, we use the Python builtin function `sorted` to return an iterable of sorted characters for each string. Then, we use `itertools.izip_longest` to iterate over the sorted characters of both strings until the end of the longer string.\n",
    "\n",
    "Here is a possible implementation using this strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'izip_longest' from 'itertools' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c165ede0941a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mizip_longest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0managram_sort_and_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mizip_longest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'izip_longest' from 'itertools' (unknown location)"
     ]
    }
   ],
   "source": [
    "from itertools import izip_longest\n",
    "\n",
    "\n",
    "def anagram_sort_and_compare(s1, s2):\n",
    "    for a, b in izip_longest(sorted(s1), sorted(s2)):\n",
    "        if a != b:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "anagram_sort_and_compare('abcde', 'edcba')  # => True\n",
    "anagram_sort_and_compare('abcde', 'abcd')  # => False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this algorithm seems like it’s $O(n)$, since there’s only one iteration to compare $n$ characters after sorting. However, the two `sorted` method calls have their own cost, typically $O(n\\log n)$. Since that function dominates $O(n)$, this algorithm will also be $O(n\\log n)$.\n",
    "\n",
    "Solution 3: Brute Force\n",
    "-----------------------\n",
    "\n",
    "A *brute force* technique for solving a problem typically tries to exhaust all possibilities. We can apply this technique to the anagram problem by generating a list of all possible strings using the characters from `s1`. If `s2` occurs in that list, then `s1` and `s2` are anagrams.\n",
    "\n",
    "There’s a problem with this approach, though: when generating all possible strings from `s1`, there are $n$ possible first characters, $n-1$ possible second characters, $n-2$ possible third characters, and so on. The total number of candidate strings is $n*(n-1)*(n-2)*...*3*2*1$, which is $n!$. Although some of the strings may be duplicates, the program won’t know that and will still generate $n!$ strings.\n",
    "\n",
    "It turns out that $n!$ grows even faster than $2^{n}$ as *n* gets large. If `s1` were 20 characters long, there would be $20!$ or\n",
    "2,432,902,008,176,640,000 possible candidate strings. If we processed one candidate every second, it would take 77,146,816,596 years to process the entire list. This is probably not going to be a good solution.\n",
    "\n",
    "Solution 4: Count and Compare\n",
    "-----------------------------\n",
    "\n",
    "Our final solution uses the fact that any two anagrams have the same number of a’s, the same number of b’s, the same number of c’s, and so on. First, we generate character counts for each string. If these counts match, the two strings are anagrams.\n",
    "\n",
    "Since there are 26 possible characters, we can use a list of 26 counters for each string. Each time we see a particular character, we’ll increment the counter at that character’s position. If the two lists are identical at the end, the strings must be anagrams.\n",
    "\n",
    "Here is a possible implementation of this strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def anagram_count_compare(s1, s2):\n",
    "    c1 = [0] * 26\n",
    "    c2 = [0] * 26\n",
    "\n",
    "    for char in s1:\n",
    "        pos = ord(char) - ord('a')\n",
    "        c1[pos] += 1\n",
    "\n",
    "    for char in s2:\n",
    "        pos = ord(char) - ord('a')\n",
    "        c2[pos] += 1\n",
    "\n",
    "    for a, b in zip(c1, c2):\n",
    "        if a != b:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "anagram_count_compare('apple', 'pleap')  # => True\n",
    "anagram_count_compare('apple', 'applf')  # => False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the solution has many iterations. However, unlike the first solution, none of them are nested. The first two iterations count characters and are $O(n)$. The third iteration always takes 26 steps since there are 26 possible characters. Adding everything gives $T(n)=2n+26$ steps, which is $O(n)$. We have found a linear order of magnitude algorithm for solving this problem.\n",
    "\n",
    "This implementation could be written more succinctly by using `collections.Counter`, which constructs a `dict`-like object mapping elements in an iterable to the number of occurrences of that element in the iterable. Behold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def anagram_with_counter(s1, s2):\n",
    "    return Counter(s1) == Counter(s2)\n",
    "\n",
    "anagram_with_counter('apple', 'pleap')  # => True\n",
    "anagram_with_counter('apple', 'applf')  # => False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `anagram_with_counter` is also $O(n)$, but we only know this because we understand the implementation of `collections.Counter`.\n",
    "\n",
    "One final thought about space requirements: although the last solution was able to run in linear time, it only did so by using additional storage for the two lists of character counts. In other words, this algorithm sacrificed space in order to gain time.\n",
    "\n",
    "This is a common tradeoff. In this case, the amount of extra space isn’t significant; however, if the underlying alphabet had millions of characters, there would be more cause for concern.\n",
    "\n",
    "On many occasions, you’ll need to choose between time and space. When given a choice of algorithms, it’s up to you as a software engineer to determine the best use of computing resources for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a general understanding of big O notation, we’re going to spend some time discussing the big O performance for the most commonly-used operations supported by Python lists and dictionaries. The efficiencies of these data types are important because we’ll be using them to implement other abstract data structures for the remainder of this book.\n",
    "\n",
    "This section is intended to give you some intuitive understanding of *why* the performances are what they are, but you won’t fully appreciate these reasons until later, when we explore how lists and dictionaries can be implemented.\n",
    "\n",
    "Keep in mind that there is a difference between the Python *language* and a [Python implementation](https://wiki.python.org/moin/PythonImplementations). Our discussion below assumes the use of the CPython implementation.\n",
    "\n",
    "Lists\n",
    "---\n",
    "\n",
    "The designers of the Python list data type had many choices to make during implementation. Each choice affected how quickly the list could perform operations. One decision they made was to optimize the list implementation for common operations.\n",
    "\n",
    "### Indexing & Assigning\n",
    "\n",
    "Two common operations are indexing and assigning to an index position. In Python lists, values are assigned to and retrieved from specific, known memory locations. No matter how large the list is, index lookup and assignment take a constant amount of time and are thus $O(1)$.\n",
    "\n",
    "### Appending & Concatenating\n",
    "\n",
    "Another common programming need is to grow a list. There are two ways to do this: you can use the `append` method or the concatenation operator (`+`).\n",
    "\n",
    "The `append` method is “amortized” $O(1)$. In most cases, the memory required to append a new value has already been allocated, which is strictly $O(1)$. Once the C array underlying the list has been exhausted, it must be expanded in order to accomodate further `append`s. This periodic expansion process is linear relative to the size of the new array, which seems to contradict our claim that `append`ing is $O(1)$.\n",
    "\n",
    "However, the expansion rate is cleverly chosen to be three times the previous size of the array; when we spread the expansion cost over each additional `append` afforded by this extra space, the cost per `append` is $O(1)$ *on an amortized basis*.\n",
    "\n",
    "On the other hand, concatenation is $O(k)$, where $k$ is the size of the concatenated list, since $k$ sequential assignment operations must occur.\n",
    "\n",
    "### Popping, Shifting & Deleting\n",
    "\n",
    "Popping from a Python list is typically performed from the end but, by passing an index, you can pop from a specific position. When `pop` is called from the end, the operation is $O(1)$, while calling `pop` from anywhere else is $O(n)$. Why the difference?\n",
    "\n",
    "When an item is taken from the front of a Python list, all other elements in the list are shifted one position closer to the beginning. This is an unavoidable cost to allow $O(1)$ index lookup, which is the more common operation.\n",
    "\n",
    "For the same reasons, inserting at an index is $O(n)$; every subsequent element must be shifted one position closer to the end to accomodate the new element. Unsurprisingly, deletion behaves the same way.\n",
    "\n",
    "### Iteration\n",
    "\n",
    "Iteration is $O(n)$ because iterating over $n$ elements requires $n$ steps. This also explains why the `in` operator in Python is $O(n)$: to determine whether an element is in a list, we must iterate over every element.\n",
    "\n",
    "### Slicing\n",
    "\n",
    "Slice operations require more thought. To access the slice `[a:b]` of a list, we must iterate over every element between indices `a` and `b`. So, slice access is $O(k)$, where $k$ is the size of the slice. Deleting a slice is $O(n)$ for the same reason that deleting a single element is $O(n)$: $n$ subsequent elements must be shifted toward the list's beginning.\n",
    "\n",
    "### Multiplying\n",
    "\n",
    "To understand list multiplication, remember that concatenation is $O(k)$, where $k$ is the length of the concatenated list. It follows that multiplying a list is $O(nk)$, since multiplying a $k$-sized list $n$ times will require $k(n - 1)$ appends.\n",
    "\n",
    "### Reversing\n",
    "\n",
    "Reversing a list is $O(n)$ since we must reposition each element.\n",
    "\n",
    "### Sorting\n",
    "\n",
    "Finally (and least intuitively), [sorting in Python](http://svn.python.org/view/python/trunk/Objects/listsort.txt?revision=69846&view=markup) is $O(n\\log{n})$ and [beyond the scope of this book](https://en.wikipedia.org/wiki/Timsort) to demonstrate.\n",
    "\n",
    "For reference, we’ve summarized the performance characteristics of\n",
    "Python's list operations in the table below:\n",
    "\n",
    "Operation  | Big O Efficiency\n",
    "--- | ---\n",
    "index []    | $O(1)$\n",
    "index assignment    | $O(1)$\n",
    "append  | $O(1)$\n",
    "pop()   | $O(1)$\n",
    "pop(i)  | $O(n)$\n",
    "insert(i, item)  | $O(n)$\n",
    "del operator    | $O(n)$\n",
    "iteration   | $O(n)$\n",
    "contains (in)   | $O(n)$\n",
    "get slice [x:y] | $O(k)$\n",
    "del slice   | $O(n)$\n",
    "reverse | $O(n)$\n",
    "concatenate | $O(k)$\n",
    "sort    | $O(n\\log{n})$\n",
    "multiply    | $O(nk)$\n",
    "\n",
    "Dictionaries\n",
    "---\n",
    "\n",
    "The second major Python data type is the dictionary. As you might recall, a dictionary differs from a list in its ability to access items by key rather than position. For now, the most important characteristic to note is that “getting” and “setting” an item in a dictionary are both $O(1)$ operations.\n",
    "\n",
    "We won't try to provide an intuitive explanation for this now, but rest assured that we’ll discuss dictionary implementations later. For now, simply remember that dictionaries were created specifically to get and set values by key as fast as possible.\n",
    "\n",
    "### `contains`\n",
    "\n",
    "Another important dictionary operation is checking whether a key is present in a dictionary. This “contains” operation is also $O(1)$ because checking for a given key is implicit in getting an item from a dictionary, which is itself $O(1)$.\n",
    "\n",
    "### Iterating & Copying\n",
    "\n",
    "Iterating over a dictionary is $O(n)$, as is copying the dictionary, since $n$ key/value pairs must be copied.\n",
    "\n",
    "We’ve summarized the efficencies of all dictionary operations in the table below:\n",
    "\n",
    "Operation  | Big O Efficiency\n",
    "--- | ---\n",
    "copy   | $O(n)$\n",
    "get item   | $O(1)$\n",
    "set item   | $O(1)$\n",
    "delete item| $O(1)$\n",
    "contains (in)  | $O(1)$\n",
    "iteration  | $O(n)$\n",
    "\n",
    "### The “Average Case”\n",
    "\n",
    "The efficiences provided in the above tables are performances in the *average case*. In rare cases, “contains”, “get item” and “set item” can degenerate into $O(n)$ performance but, again, we’ll discuss that when we talk about different ways of implementing a dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "Python is still an evolving language, which means that the above tables could be subject to change. The latest information on the performance of Python data types can be found on the Python website. As of this writing, the Python wiki has a nice time complexity page that can be found at the [Time Complexity Wiki](http://wiki.python.org/moin/TimeComplexity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
